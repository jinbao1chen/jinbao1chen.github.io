---
layout: post
title: "网络爬虫-requests模块"
description: 网络爬虫，requests模块
date: 2021-09-20
tag: 爬虫，python
---

## requests模块
1、requests模块作用：发送HTTP请求，获取响应数据

2、安装：pip/pip3 install requests 或 在pycharm安装, File-->Setting-->Project:xx-->Python Interpreter-->Package--> "+" >> 输入 requests 直接安装即可。

3、使用，发送get请求：（1）导入；（2）调用get方法，对目标URL发送请求

## 响应对象
分类：

源码      XX.text       str字符串

二进制    .content      bytes

```
例子：
# 导入
import requests

# 目标网址
url = 'http://www.baidu.com'

# 发出请求
response = requests.get(url)

# 设置（响应）解码格式
response.encoding = 'utf-8'

# 打印源码的str类型数据
print(response.text)

# print(response.encoding)

# response.content 是存储的bytes类型的响应源码，可以进行decode操作
print(response.content.decode()) #默认utf-8
```

## 发送带请求头headers的请求
requests.get(url, headers={})
```
例子：
import requests

url = 'http://www.baidu.com'
response = requests.get(url)

# 构建请求头字典
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'
}

# 发送带请求头的请求
response1 = requests.get(url, headers=headers)

print(len(response1.content.decode()))
print(response1.content.decode())
```

## 发送带参数的请求
1、url中直接带参数

2、使用params参数：

（1）构建参数字典

（2）发送请求的时候设置参数字典
```
# 知识点：掌握发送带参数的请求的方法

import requests

# 发送带参数的请求两种方法：（1）直接在url中加入参数

##############################################################################################
# # 第一种方法
# url = 'https://www.baidu.com/s?&wd=python'  # 通过删除法获得的url
#
# # 构建请求头字典
# headers = {
#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'
# }
#
# response = requests.get(url,headers=headers)
#
# # 验证
# with open('baidu.html','wb') as f:
#     f.write(response.content)

###############################################################################################
# 第二种方法：通过参数params携带参数字典
# 1、构建参数字典
# 2、发送请求的时候设置参数字典
url = 'https://www.baidu.com/s?'

# 构建请求头字典
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'
}

# 构建参数字典
data = {
    'wd':'python'
}

response = requests.get(url,headers=headers,params=data)

# 验证
with open('baidu1.html','wb') as f:
    f.write(response.content)
```

## 在headers中携带cookies
方法同ua
```
# 知识点：掌握headers中携带cookies
# 对于登录，需要cookies保持登录状态。

import requests

url = 'https://github.com/jinbao1chen'

headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36',
    'cookie': '_octo=GH1.1.1844290650.1631717716; tz=Asia%2FShanghai; _device_id=85f612f7b15e03a1c45b200d88608722; has_recent_activity=1; user_session=46oFNc-72H_aOTyzPXbbWHoffxbh8W8THzEcFxIrQdzXKIs9; __Host-user_session_same_site=46oFNc-72H_aOTyzPXbbWHoffxbh8W8THzEcFxIrQdzXKIs9; tz=Asia%2FShanghai; color_mode=%7B%22color_mode%22%3A%22auto%22%2C%22light_theme%22%3A%7B%22name%22%3A%22light%22%2C%22color_mode%22%3A%22light%22%7D%2C%22dark_theme%22%3A%7B%22name%22%3A%22dark%22%2C%22color_mode%22%3A%22dark%22%7D%7D; logged_in=yes; dotcom_user=jinbao1chen; _gh_sess=NgmBBHrLFMpZrWTqbdfIKFouWCUC77eL%2Fd5vmgL3SfOjrXPwnks8sKrgLfx%2FN%2Bu77Rqvv%2FZ6fdpP881nVG6ebFlBl2qnhv3vKWVhW8wNHhdee1JcjiIdCHlpVe9f%2B6m6U0pgA8a3NQYEalEkCw4mNePnAv2MNvpyQJasxhd4UTqPeiKcmNLffllvDIRRHpNmn9TE3qYCbTY8Z14jU39XH7qmcJ81uwZ6Z0juhOHwvfkLtpqoJybkSaUhEORCGpdULZX%2FaZBdkcRZXdnivgOJ5%2FxapuxiaM1gbzofGjfP%2FZKna2BZCTXy1%2BlnY21wcQkI4cT3JDS40O%2BAjEq%2FNEFlgQH3t6oAoPIdiukLvC0F27BC2HZehqmcUfLjLz%2BV%2FQ2G%2FJbLAJ%2F%2BpJjS0hmoHL%2Fmp7EjNLOF0oU%2FGyJoonIk0VW0NaA%2FS9LxunBCHVfej23wolzfh9KYbE37kCDY3nYz266NP0eK57Q4rfcCvlDsb2uClpmSJIuanx%2BZBI2Lr9LLyYbmidS25X8MfAEueTzQXSNOh02BMdbJbEd87%2BHPIOqmMvcGDJ3%2F%2BehOvbDJJgPsOS3QpLhhc5zxrE7L9GkTwOAfgRQi3OkhvRobrm1%2FSwhg24brVnVlf4Xn%2BxDivc5M%2FbXoDsAmeqjC77d210r%2BhN6KCsTrR5UnYQsnMD4V4QlvTtgpgxWIV1M27VeXHl0LGIoIJRdbWEKmDxX4IgoVZD7XRJov%2BOvZ52OV0xjoxMFFJfyf4vfKEEQTrB1BNM8v61foCFJLLGGwekrgW6Pn4vIOiF5lMYzYiWQyzO2a22%2BLAIoqxYk4wRBblNIB7v6v3Ro%2Fm%2FeR0HZMNTKGtG0aBJembSVF6ywH6EDqgn%2F%2F1EC7ICslk4vrGWlUig2b%2F%2FBIFx2fekfQZ3N0wkgwxtgG4f03LujVyLnDlPulLw%3D%3D--IvqopqaZJvu9dWfC--JlES5vUzbHS9Z34fitlTEg%3D%3D'
}

response = requests.get(url,headers=headers)

# 验证
with open('github.html','wb')as f:
    f.write(response.content)
```

## 使用cookies参数保持会话
1、构建cookies字典

2、在请求的时候将cookies字典赋值给cookies参数：requests.get(url, cookies)
```
# 知识点：掌握cookies参数的使用
# 使用cookies参数保持会话：构建cookies字典；在请求的时候将cookies字典赋值给cookies参数，requests.get(url,cookies)

import requests

url = 'https://github.com/jinbao1chen'

headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'
}

# 构建cookies字典
temp = '_octo=GH1.1.1844290650.1631717716; tz=Asia%2FShanghai; _device_id=85f612f7b15e03a1c45b200d88608722; has_recent_activity=1; user_session=46oFNc-72H_aOTyzPXbbWHoffxbh8W8THzEcFxIrQdzXKIs9; __Host-user_session_same_site=46oFNc-72H_aOTyzPXbbWHoffxbh8W8THzEcFxIrQdzXKIs9; tz=Asia%2FShanghai; color_mode=%7B%22color_mode%22%3A%22auto%22%2C%22light_theme%22%3A%7B%22name%22%3A%22light%22%2C%22color_mode%22%3A%22light%22%7D%2C%22dark_theme%22%3A%7B%22name%22%3A%22dark%22%2C%22color_mode%22%3A%22dark%22%7D%7D; logged_in=yes; dotcom_user=jinbao1chen; _gh_sess=NgmBBHrLFMpZrWTqbdfIKFouWCUC77eL%2Fd5vmgL3SfOjrXPwnks8sKrgLfx%2FN%2Bu77Rqvv%2FZ6fdpP881nVG6ebFlBl2qnhv3vKWVhW8wNHhdee1JcjiIdCHlpVe9f%2B6m6U0pgA8a3NQYEalEkCw4mNePnAv2MNvpyQJasxhd4UTqPeiKcmNLffllvDIRRHpNmn9TE3qYCbTY8Z14jU39XH7qmcJ81uwZ6Z0juhOHwvfkLtpqoJybkSaUhEORCGpdULZX%2FaZBdkcRZXdnivgOJ5%2FxapuxiaM1gbzofGjfP%2FZKna2BZCTXy1%2BlnY21wcQkI4cT3JDS40O%2BAjEq%2FNEFlgQH3t6oAoPIdiukLvC0F27BC2HZehqmcUfLjLz%2BV%2FQ2G%2FJbLAJ%2F%2BpJjS0hmoHL%2Fmp7EjNLOF0oU%2FGyJoonIk0VW0NaA%2FS9LxunBCHVfej23wolzfh9KYbE37kCDY3nYz266NP0eK57Q4rfcCvlDsb2uClpmSJIuanx%2BZBI2Lr9LLyYbmidS25X8MfAEueTzQXSNOh02BMdbJbEd87%2BHPIOqmMvcGDJ3%2F%2BehOvbDJJgPsOS3QpLhhc5zxrE7L9GkTwOAfgRQi3OkhvRobrm1%2FSwhg24brVnVlf4Xn%2BxDivc5M%2FbXoDsAmeqjC77d210r%2BhN6KCsTrR5UnYQsnMD4V4QlvTtgpgxWIV1M27VeXHl0LGIoIJRdbWEKmDxX4IgoVZD7XRJov%2BOvZ52OV0xjoxMFFJfyf4vfKEEQTrB1BNM8v61foCFJLLGGwekrgW6Pn4vIOiF5lMYzYiWQyzO2a22%2BLAIoqxYk4wRBblNIB7v6v3Ro%2Fm%2FeR0HZMNTKGtG0aBJembSVF6ywH6EDqgn%2F%2F1EC7ICslk4vrGWlUig2b%2F%2FBIFx2fekfQZ3N0wkgwxtgG4f03LujVyLnDlPulLw%3D%3D--IvqopqaZJvu9dWfC--JlES5vUzbHS9Z34fitlTEg%3D%3D'

# 切割成键值对
# 1、稳妥方案
# cookie_list = temp.split('; ')  #以 分号+空格 为标记分割
# cookies = {}   #创建一个cookies字典
#
# for cookie in cookie_list:
#     cookies[cookie.split('=')[0]] = cookie.split('=')[-1]  #获取等号右边的值

# 2、优化方案(字典推导式写得特别6)
cookie_list = temp.split('; ')
cookies = {cookie.split('=')[0]: cookie.split('=')[-1] for cookie in cookie_list}

print(cookies)

response = requests.get(url,headers=headers,cookies=cookies)

# 验证
with open('github_with_cookies.html','wb')as f:
    f.write(response.content)
```

```
# requests模块：cookiejar对象处理
# cookiejar对象转换为cookies字典的方法：1、转换方法 cookies_dict = requests.utils.dict_from_cookiejar(response.cookies)
# 2、其中 response.cookies 返回的就是cookiejar类型的对象 3、requests.utils.dict_from_cookiejar 函数返回cookies字典
# 4、requests.utils.cookiejar_from_dict 函数返回字典的cookiejar类型的对象，但会丢失域名

import requests

url = 'http://www.baidu.com'

response = requests.get(url)
print(response.cookies)

dict_cookies = requests.utils.dict_from_cookiejar(response.cookies)
print(dict_cookies)

jar_cookies = requests.utils.cookiejar_from_dict(dict_cookies)
print(jar_cookies)
```

## 超时参数timeout的使用
requests.get(url, timeout=3)
```
# timeout 参数的使用

import requests

url = 'https://twitter.com'

response = requests.get(url,timeout=3) #3秒连接不成功即返回失败，防止程序阻塞
```

## 代理
1、什么是代理：代理IP是一个ip,指向的是一个代理服务器,通常用作应对网站反爬

2、分类：正向代理和反向代理，以知不知道最终服务器的地址为判断标准

3、代理IP的分类：（1）按匿名度，透明、匿名、高匿 （2）协议，http、https、socks

4、使用：requests.get(url, proxies=proxies)

代理使用成功不会有任何报错，能成功获取响应；如果失败，要么卡滞，要么报错
```
# 代理：是一个ip，指向的是一个代理服务器
# 正向代理和反向代理，以知不知道最终服务器的地址作为判断标准
# 代理分类：透明、匿名、高匿，服务器能否判断出代理为标准，使用高匿代理效果最好
# 协议分类：http、https、socks
# 用法：response = requests.get(url, proxies=proxies), proxies形式是字典，例如：
# proxies = { 'http' : 'http://12.34.56.79:9527',
#           'https' : 'http://12.34.56.79:9527' }
# 如果proxies字典中包含多个键值对，发送请求时将按照url地址的协议来选择使用响应的代理ip

# 代理使用成功不会有任何报错，能成功获取响应；如果失败，要么卡滞，要么报错

import requests

url = 'https://www.baidu.com/'

# 构建代理字典，代理从网上搜免费，但是需要甄别筛选
proxies = {
    'http': 'http://36.56.100.109:9999',
    # 'https': 'https://223.240.208.230:1133'
}

# response = requests.get(url)
response = requests.get(url, proxies=proxies)
print(response.text)
```

## CA认证
```
# 爬虫过程遇到 网站的CA证书没有经过认证，会导致连接不是私密链接，无法进入。解决：无视忽略CA证书

import requests

url = 'https://sam.huat.edu.cn:8443/selfservice'

response = requests.get(url, verify=False) # 直接添加参数，有警告但无碍

print(response.content)
```

## requests模块发送post请求
1、实现方法：requests.post(url, data), data是一个字典

2、post数据来源 <<重点/难点>>

(1)固定值：           抓包比较不变值

(2)输入值：           抓包比较根据自身变化值

(3)预设值-静态文件：  需要提前从静态html中获取

(4)预设值-发请求：    需要指定地址发送请求

(5)在客户端生成的：   分析js，模拟生成数据
```
# requests模块发送 post 请求
# 方法：response = requests.post(url,data), data 参数接收一个字典
# requests 模块发送post请求函数的其他参数和发送 get 请求的参数完全一致

import json
import sys
import requests

class King(object):

    def __init__(self,word):
        self.url = 'http://ifanyi.iciba.com/index.php?c=trans&m=fy&client=6&auth_user=key_ciba&sign=37218aa29f55fdcc'

        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'
        }

        self.data = {
            'from': 'zh',
            'to': 'en',
            'q': word
        }

    def get_data(self):
        # 使用post发送一个请求
        reponse = requests.post(self.url, data=self.data, headers=self.headers)
        return reponse.content

    def data_param(self,data):
        # loads方法将json字符串转化成python字典
        dict_data = json.loads(data)
        print(dict_data['content']['out'])

    # 编写爬虫逻辑
    def run(self):
        # url
        # headers
        # data字典
            # post数据来源：（难点）
            # 1、固定值：        抓包比较不变值
            # 2、输入值：        抓包比较根据自身变化值
            # 3、预设值-静态文件： 需要提前从静态html中获取
            # 4、预设值-发请求：  需要对指定地址发送请求
            # 5、在客户端生成：   分析js，模拟生成数据
        # 发送请求获取响应
        reponse = self.get_data()
        # print(reponse)
        # 数据解析
        self.data_param(reponse)

if __name__=='__main__':
    # print(sys.argv)
    # word = input('请输入要翻译的单词或句子：')
    word = sys.argv[1]
    king = King(word)
    king.run()
```

## requests.session 进行状态保持
1、作用：自动处理cookies

2、场景：连续的多次请求

3、使用方法：
```
session = requests.session                      #实例化session
response = session.get(url, headers=headers)    #调用session方法发送get请求
response = session.post(url, data, headers)
```

```
# 知识点：掌握 利用 requests.session 进行状态保持
# 作用：自动处理cookie
# 场景：连续的多次请求
# 使用方法：先利用requests创建session的类，然后借助session类调用get或者post发送请求

# 以 GitHub登陆 为例

import requests
import re

def login():
    # session，创建session对象
    session = requests.session()

    # headers
    session.headers = {
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'
    }

    # url1-获取token
    url1 = 'https://github.com/login'
        # 发送请求获取响应
    res_1 = session.get(url1).content.decode()
        # 正则表达式提取token
    token = re.findall('name="authenticity_token" value="(.*?)" /> ', res_1)
          # 'name="authenticity_token" value="z/c5WeKPQcc9nQ0wECMnJGKtE3zPtzT85J3QQ/+54ejFtUQW0xkwuA3XA1GA85DTnN4VkXp4i0AYd2iboS4rXw==" /> '
    # print(token)

    # url2-登陆
    url2 = 'https://github.com/session'
        # 构建表单数据
    data = {
        "commit": "Sign in",
        "authenticity_token": token,
        "login": "ch_cjb@163.com",
        "password": "******",  #密码隐藏
        "webauthn-support": "supported",
        "webauthn-iuvpaa-support": "unsupported",
        "return_to": "https://github.com/login"
    }
    # print(data)
        # 发送请求登录
    session.post(url2, data=data)

    # url3-验证
    url3 = 'https://github.com/jinbao1chen'
    response = session.get(url3)
    with open('github_session.html','wb') as f:
        f.write(response.content)

if __name__ == '__main__':
    login()
```
